{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RescueTime EDA - 2023 Edition</h1>\n",
    "\n",
    "<i>Author: Steven Yuan</i>\n",
    "\n",
    "**Goal:** Find useful patterns in device usage, and devise strategies to break those patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_url = 'https://www.rescuetime.com/anapi/data'\n",
    "\n",
    "key = 'B63Vi3QcJOvhpM4PHXaQLh_HxHZmevm3CxP2FNBF'\n",
    "file_format = 'csv'  # 'json' or 'csv'\n",
    "perspective = 'interval'\n",
    "interval = 'hour'\n",
    "restrict_begin = '2023-10-01'\n",
    "restrict_end = '2023-12-31'\n",
    "restrict_kind = 'activity'\n",
    "strip_dash = lambda s: s.replace('-', '')\n",
    "\n",
    "payload = {\n",
    "    'key': key, \n",
    "    'format': file_format, \n",
    "    'perspective': perspective,\n",
    "    'interval': interval, \n",
    "    'restrict_begin': restrict_begin,\n",
    "    'restrict_end': restrict_end,\n",
    "    'restrict_kind': restrict_kind\n",
    "}\n",
    "rt_data_file = '../rt_data/rt_data_{}-{}-{}_{}-{}.{}'.format(\n",
    "        perspective, interval, restrict_kind,\n",
    "        strip_dash(restrict_begin), strip_dash(restrict_end),\n",
    "        file_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-12-31'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(date.fromisoformat('2020-01-01') - timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters = ['2021-01-01', '2021-04-01', '2021-07-01', '2021-10-01', '2022-01-01', '2022-04-01', '2022-07-01', '2023-01-01', '2023-04-01', '2023-07-01', '2024-01-01']\n",
    "for begin, end in zip(quarters, quarters[1:]):\n",
    "    payload = {\n",
    "        'key': key, \n",
    "        'format': file_format, \n",
    "        'perspective': perspective,\n",
    "        'interval': interval, \n",
    "        'restrict_begin': begin,\n",
    "        'restrict_end': restrict_end,\n",
    "        'restrict_kind': restrict_kind\n",
    "    }\n",
    "    rt_data_file = '../rt_data/rt_data_{}-{}-{}_{}-{}.{}'.format(\n",
    "            perspective, interval, restrict_kind,\n",
    "            strip_dash(restrict_begin), strip_dash(restrict_end),\n",
    "            file_format\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Date,Time Spent (seconds),Number of People,Activity,Category,Productivity\\n2023-10-02T09:00:00,1304,1,Distracted (offline),General Business,-2\\n2023-10-02T10:00:00,3600,1,Distracted (offline),General Business,-2\\n2023-10-02T11:00:00,3600,1,Distracted (offline),General Business,-2\\n2023-10-02T12:00:00,3600,1,Distracted (offline),General Business,-2\\n2023-10-02T13:00:00,963,1,Distracted (offline),General Business,-2\\n2023-10-02T13:00:00,511,1,qems2.grapesmoker.net,General Reference & Learning,2\\n2023-10-02T13:00:00,309,1,pycharm,Editing & IDEs,2\\n2023-10-02T13:00:00,70,1,docs.python.org,General Software Development,2\\n2023-10-02T13:00:00,18,1,safari-resource:,Browsers,0\\n2023-10-02T13:00:00,9,1,RescueTime,Intelligence,2\\n2023-10-02T13:00:00,6,1,Google Documents,Writing,2\\n2023-10-02T13:00:00,5,1,Safari,Browsers,0\\n2023-10-03T13:00:00,3320,1,Distracted (offline),General Business,-2\\n2023-10-03T14:00:00,2730,1,Distracted (offline),General Business,-2\\n2023-10-03T14:00:00,328,1,gpt-index.readthedocs.io,Uncategorized,0\\n2023-10-03T14:00:00,150,1,developer.poe.com,General Software Development,2\\n2023-10-03T14:00:00,125,1,platform.openai.com,General Reference & Learning,2\\n2023-10-03T14:00:00,111,1,nijianmo.github.io,General Software Development,2\\n2023-10-03T14:00:00,58,1,pycharm,Editing & IDEs,2\\n2023-10-03T14:00:00,42,1,Safari,Browsers,0\\n2023-10-03T14:00:00,28,1,Github,General Software Development,2\\n2023-10-03T14:00:00,8,1,Google Documents,Writing,2\\n2023-10-03T14:00:00,4,1,google.com,Search,1\\n2023-10-03T14:00:00,4,1,RescueTime,Intelligence,2\\n2023-10-03T14:00:00,1,1,openai.com,General Reference & Learning,2\\n2023-10-03T15:00:00,1164,1,arxiv.org,Search,1\\n2023-10-03T15:00:00,250,1,Safari,Browsers,0\\n2023-10-03T15:00:00,167,1,gpt-index.readthedocs.io,Uncategorized,0\\n2023-10-03T15:00:00,5,1,Github,General Software Development,2\\n2023-10-03T15:00:00,3,1,platform.openai.com,General Reference & Learning,2\\n2023-10-03T15:00:00,2,1,medium.com,General News & Opinion,-2\\n2023-10-03T17:00:00,721,1,rescuetime.com,Computer,1\\n2023-10-03T17:00:00,676,1,pycharm,Editing & IDEs,2\\n2023-10-03T17:00:00,266,1,localhost:8888,General Software Development,2\\n2023-10-03T17:00:00,227,1,requests.readthedocs.io,Engineering & Technology,2\\n2023-10-03T17:00:00,160,1,datacamp.com,Engineering & Technology,2\\n2023-10-03T17:00:00,135,1,help.rescuetime.com,General Reference & Learning,2\\n2023-10-03T17:00:00,108,1,developer.mozilla.org,Engineering & Technology,2\\n2023-10-03T17:00:00,78,1,Safari,Browsers,0\\n2023-10-03T17:00:00,74,1,arxiv.org,Search,1\\n2023-10-03T17:00:00,53,1,google.com,Search,1\\n2023-10-03T17:00:00,41,1,urllib3.readthedocs.io,Engineering & Technology,2\\n2023-10-03T17:00:00,38,1,jetbrains.com,Search,1\\n2023-10-03T17:00:00,38,1,docs.python.org,General Software Development,2\\n2023-10-03T17:00:00,22,1,stackoverflow.com,General Software Development,2\\n2023-10-03T17:00:00,20,1,Mail,Email,1\\n2023-10-03T17:00:00,10,1,freecodecamp.org,Uncategorized,0\\n2023-10-03T17:00:00,8,1,pypi.org,General Reference & Learning,2\\n2023-10-03T17:00:00,8,1,Github,General Software Development,2\\n2023-10-03T17:00:00,8,1,lilianweng.github.io,General Software Development,2\\n2023-10-03T17:00:00,6,1,feedback.userreport.com,General Business,2\\n2023-10-03T17:00:00,3,1,RescueTime,Intelligence,2\\n2023-10-03T17:00:00,2,1,Dictionary,General Reference & Learning,2\\n2023-10-03T17:00:00,1,1,users,General Software Development,2\\n2023-10-03T18:00:00,517,1,pycharm,Editing & IDEs,2\\n2023-10-03T18:00:00,117,1,stackoverflow.com,General Software Development,2\\n2023-10-03T18:00:00,66,1,localhost:8888,General Software Development,2\\n2023-10-03T18:00:00,51,1,Terminal,Systems Operations,2\\n2023-10-03T18:00:00,39,1,google.com,Search,1\\n2023-10-03T18:00:00,34,1,system settings,Computer,1\\n2023-10-03T18:00:00,4,1,Safari,Browsers,0\\n2023-10-03T18:00:00,4,1,Finder,Computer,1\\n2023-10-03T18:00:00,3,1,rescuetime.com,Computer,1\\n2023-10-03T18:00:00,3,1,requests.readthedocs.io,Engineering & Technology,2\\n2023-10-03T18:00:00,2,1,deviantart.com,General Entertainment,-2\\n2023-10-03T18:00:00,1,1,users,General Software Development,2\\n2023-10-03T18:00:00,1,1,docs.python.org,General Software Development,2\\n2023-10-03T23:00:00,5,1,system settings,Computer,1\\n2023-10-03T23:00:00,1,1,Finder,Computer,1\\n2023-10-04T10:00:00,288,1,Finder,Computer,1\\n2023-10-04T10:00:00,68,1,Mail,Email,1\\n2023-10-04T10:00:00,22,1,system settings,Computer,1\\n2023-10-04T10:00:00,3,1,discord,General Entertainment,-2\\n2023-10-04T11:00:00,57,1,Terminal,Systems Operations,2\\n2023-10-04T11:00:00,30,1,myhealthchart.com,Uncategorized,0\\n2023-10-04T11:00:00,25,1,rescuetime.com,Computer,1\\n2023-10-04T11:00:00,20,1,google.com,Search,1\\n2023-10-04T11:00:00,17,1,johnmuirhealth.com,Uncategorized,0\\n2023-10-04T11:00:00,4,1,Safari,Browsers,0\\n2023-10-04T11:00:00,1,1,stackoverflow.com,General Software Development,2\\n2023-10-04T11:00:00,1,1,Finder,Computer,1\\n2023-10-04T12:00:00,75,1,johnmuirhealth.com,Uncategorized,0\\n2023-10-04T12:00:00,55,1,Mail,Email,1\\n2023-10-04T12:00:00,42,1,Terminal,Systems Operations,2\\n2023-10-04T12:00:00,21,1,install command line developer tools,Systems Operations,2\\n2023-10-04T12:00:00,3,1,Finder,Computer,1\\n2023-10-04T12:00:00,2,1,lastpass,General Utilities,1\\n2023-10-04T12:00:00,2,1,Safari,Browsers,0\\n2023-10-04T16:00:00,511,1,Terminal,Systems Operations,2\\n2023-10-04T16:00:00,6,1,google.com,Search,1\\n2023-10-04T16:00:00,5,1,stackoverflow.com,General Software Development,2\\n2023-10-04T16:00:00,4,1,Safari,Browsers,0\\n2023-10-04T16:00:00,2,1,Finder,Computer,1\\n2023-10-04T16:00:00,1,1,install command line developer tools,Systems Operations,2\\n2023-10-04T17:00:00,1758,1,localhost:8888,General Software Development,2\\n2023-10-04T17:00:00,663,1,rescuetime.com,Computer,1\\n2023-10-04T17:00:00,163,1,Terminal,Systems Operations,2\\n2023-10-04T17:00:00,113,1,requests.readthedocs.io,Engineering & Technology,2\\n2023-10-04T17:00:00,107,1,docs.python.org,General Software Development,2\\n2023-10-04T17:00:00,105,1,w3schools.com,Engineering & Technology,2\\n2023-10-04T17:00:00,92,1,geeksforgeeks.org,Operations,2\\n2023-10-04T17:00:00,83,1,Safari,Browsers,0\\n2023-10-04T17:00:00,82,1,google.com,Search,1\\n2023-10-04T17:00:00,81,1,pycharm,Editing & IDEs,2\\n2023-10-04T17:00:00,56,1,system settings,Computer,1\\n2023-10-04T17:00:00,30,1,scaler.com,Uncategorized,0\\n2023-10-04T17:00:00,24,1,stackoverflow.com,General Software Development,2\\n2023-10-04T17:00:00,14,1,datacamp.com,Engineering & Technology,2\\n2023-10-04T17:00:00,6,1,safari-resource:,Browsers,0\\n2023-10-04T17:00:00,2,1,Finder,Computer,1\\n2023-10-04T17:00:00,1,1,datatofish.com,Uncategorized,0\\n2023-10-04T17:00:00,1,1,users,General Software Development,2\\n2023-10-04T18:00:00,428,1,localhost:8888,General Software Development,2\\n2023-10-04T18:00:00,117,1,docs.python.org,General Software Development,2\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_data_raw = requests.get(rt_url, params=payload)\n",
    "rt_data_raw.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rt_data_file, 'w') as f:\n",
    "    f.write(rt_data_raw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{\"error\":\"# unsupported request\"</th>\n",
       "      <th>messages:\"Reports by hour are limited to a time span of 3 months.\"}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [{\"error\":\"# unsupported request\", messages:\"Reports by hour are limited to a time span of 3 months.\"}]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_all = pd.read_csv(rt_data_file)\n",
    "rt_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into two csv files: `rt_data_1.csv`, which contains all computer and offline time from January 1, 2018 to November 30, 2020; and `rt_mobile_data_1.csv`, which contains all mobile time over that same period. Both files have six columns of data, which are\n",
    "1. `Date`: A string representing a timestamp. Contains the day and hour.\n",
    "2. `Time Spent (seconds)`: An integer representing the number of seconds spent on each activity.\n",
    "3. `Number of People`: An integer representing the number of people involved in each activity. For all rows in our data sets, this is equal to 1.\n",
    "4. `Activity`: A string representing the name of each activity, which can be a website e.g. `reddit.com`, or an appplication e.g. `Firefox`.\n",
    "5. `Category`: A string representing the category each activity belongs in (see [here](https://help.rescuetime.com/article/65-what-are-categories-and-how-are-they-assigned) for information about how categories work in RescueTime).\n",
    "6. `Productivity`: An integer from -2 to 2 inclusive representing the \"productivity score\" of each activity. The scale is as follows: -2 = Very Distracting, -1 = Distracting, 0 = Neutral, 1 = Productive, 2 = Very Productive.\n",
    "\n",
    "We load `rt_data_1.csv` into a pandas DataFrame called `rt_computer_original` and `rt_mobile_data_1.csv` into `rt_mobile_original`. We also create `rt_all_original`, which contains all activities from both devices, by appending `rt_mobile_original` to `rt_computer_original` and sorting by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_computer_original = pd.read_csv(\"rt_data/rt_data_1.csv\")\n",
    "rt_mobile_original = pd.read_csv(\"rt_data/rt_mobile_data_1.csv\")\n",
    "rt_all_original = rt_computer_original.append(rt_mobile_original, ignore_index=True).sort_values(\"Date\", ignore_index=True)\n",
    "display(rt_computer_original.head(10), rt_mobile_original.head(10), rt_all_original.tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull up information about the structure of both data sets to corroborate our observations from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_computer_original.info(verbose=True)\n",
    "print()\n",
    "rt_mobile_original.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do some data cleaning. First, we find the activities that were most logged in our computer data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rt_computer_original[\"Activity\"].value_counts().head(15)) # Displays unique values in the \"Activity\" columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are many observations for something called \"loginwindow,\" as well as many for apparently blank webpages like \"newtab\" and \"Blank Web Browser.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rt_computer_original.query(\"Activity in ['loginwindow', 'about:newtab', 'Blank Web Browser', 'newtab']\").sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loginwindow` background task on MacOS manages the behavior of the login screen, so time spent \"using\" this application does not say anything significant about our overall pattern of device usage. Thus, we'll remove any rows that list their activity as \"loginwindow\" or any of the blank webpages.\n",
    "\n",
    "(Of course, there could be many more background tasks or non-significant activities in our data set, but going through and taking out every single one is a big pain, and these hypothetical tasks don't seem to show up all that frequently anyway. So leaving those in shouldn't affect our analysis or the conclusion we draw.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_computer_mod = rt_computer_original.query(\"Activity not in ['loginwindow', 'about:newtab', 'newtab', 'Blank Web Browser']\").reset_index(drop=True)\n",
    "rt_all_mod = rt_all_original.query(\"Activity not in ['loginwindow', 'about:newtab', 'newtab', 'Blank Web Browser']\").reset_index(drop=True)\n",
    "rt_computer_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the observations in our `rt_mobile_original` data set need to be removed (they all represent meaningful time spent), so next we extract useful aspects of our data. \n",
    "\n",
    "The following method `clean_rt_data()` takes in any DataFrame having the same six columns as `rt_computer_original` or `rt_mobile_original` and returns a new DataFrame with the following columns:\n",
    "1. `dt`, which contains DateTime objects converted from the `Date` column\n",
    "2. `act`, which contains the `Activity` column\n",
    "3. `cat`, which contains the `Category` column\n",
    "4. `prod`, which contains the `Productivity` column\n",
    "5. `mins`, which contains the `Time Spent (seconds)` column converted to minutes i.e. divided by 60\n",
    "6. `hrs`, which contains the `Time Spent (seconds)` column converted to hours i.e. divided by 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rt_data(rt_data):\n",
    "    return pd.DataFrame({'dt' : pd.to_datetime(rt_data[\"Date\"]), \n",
    "                         'act' : rt_data[\"Activity\"],\n",
    "                         'cat' : rt_data[\"Category\"],\n",
    "                         'prod' : rt_data[\"Productivity\"],\n",
    "                         'mins' : rt_data[\"Time Spent (seconds)\"] / 60,\n",
    "                         'hrs' : rt_data[\"Time Spent (seconds)\"] / 3600})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, we can easily create cleaned copies of all of our original data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned data sets from original data\n",
    "rt_computer_clean = clean_rt_data(rt_computer_mod)\n",
    "rt_mobile_clean = clean_rt_data(rt_mobile_original)\n",
    "rt_all_clean = clean_rt_data(rt_all_mod)\n",
    "\n",
    "display(rt_computer_clean.head(10), rt_mobile_clean.head(10), rt_all_clean.tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data shows how much time we spend on each individual activity every hour that RescueTime has recorded data for. If we want to see the *total* amount of time spent per hour, we will need to group the data by the date and time in the `dt` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_gb(rt_data, index=False):\n",
    "    '''\n",
    "    Groups cleaned RescueTime data by the `dt` column, and aggregates by sum. \n",
    "    This gives us the total amount of time spent on devices per hour.\n",
    "    '''\n",
    "    return rt_data.groupby('dt', as_index=index).agg(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rt_all_hourly = hourly_gb(rt_all_clean[['dt', 'mins', 'hrs']])\n",
    "display(rt_all_hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to plot the average amount of time per day I spent on devices tracked by RescueTime for every month that we have data. To do this, we do two groupings: first, we group by the date and aggregate by sum to find the total amount of time logged by RescueTime every day; then, we group by the month and year and aggregate by average to find the average amount of time logged per day for each month. \n",
    "\n",
    "To do this, we define a helper method that extracts several quantities from the `dt` column and appends them as columns. They are:\n",
    "1. `date`, the date\n",
    "2. `hour`, the hour (12am=0, 1am=1, etc.)\n",
    "3. `dow`, the day of the week (Su=0, M=1, etc.)\n",
    "4. `week`, the week ordinal (first week=1, second week=2, etc.)\n",
    "5. `month`, the month (Jan=1, Feb=2, etc.)\n",
    "6. `quarter`, the quarter (Jan-Mar=1, Apr-Jun=2, etc.)\n",
    "7. `year`, the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dt_stats(rt_data):\n",
    "    '''\n",
    "    Given RescueTime data with a `dt` column, returns a DataFrame with statistics about each timestamp in the `dt` column\n",
    "    '''\n",
    "    return rt_data.assign(date=rt_data['dt'].dt.date,\n",
    "                          hour=rt_data['dt'].dt.hour,\n",
    "                          dow=rt_data['dt'].dt.weekday,\n",
    "                          week=rt_data['dt'].dt.isocalendar().week,\n",
    "                          month=rt_data['dt'].dt.month,\n",
    "                          quarter=rt_data['dt'].dt.quarter,\n",
    "                          year=rt_data['dt'].dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rt_all_hourly_dt = add_dt_stats(rt_all_hourly) # Add DateTime statistics to our hourly data\n",
    "display(rt_all_hourly_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the month and year values do not get summed, we first group by `date`, `month`, and `year`, which is equivalent to just grouping by `date` but with extra information attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rt_all_day_month = rt_all_hourly_dt.groupby(['date', 'month', 'year'], as_index=False).agg(np.sum)[['date', 'month', 'year', 'mins', 'hrs']]\n",
    "display(rt_all_day_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we group by month and year, aggregating using the average function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rt_all_month_avg = rt_all_day_month.groupby(['month', 'year'], as_index=False).agg(np.mean)[['month', 'year', 'hrs']]\n",
    "rt_all_month_avg = rt_all_month_avg.sort_values(['year', 'month'], ignore_index=True) # Sort by year, then by month, to get data into chronological order\n",
    "rt_all_month_avg = rt_all_month_avg.assign(month_str=pd.to_datetime(rt_all_month_avg[['month', 'year']].assign(day=np.ones(rt_all_month_avg.shape[0])))) # Create a \"month string\" consisting of the month and year concatenated together\n",
    "display(rt_all_month_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this data by plotting it as a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot('month_str', 'hrs', data=rt_all_month_avg)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization seems to show that as time goes on, I'm spending more and more time per day on my electronic devices. But, there are two possible confounding variables that need to be addressed:\n",
    "1. This visualization is of *all* time tracked by RescueTime, including mobile time. I started tracking my mobile time many months after I first started tracking time on my computer (see below), so the uptick in the amount of time could just be due to more accurate time tracking.\n",
    "2. RescueTime does not log specific hours if no device usage was detected during that hour. While this could be due to RescueTime not being active for that specific hour (indeed, there are many stretches in my data set where nothing was recording because I forgot to turn RescueTime back on after it had terminated), this could also be because I genuinely did not use my computer or phone during those time periods. Thus, my true daily average per month should be lower than the above analysis indicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.min(rt_mobile_clean[['dt']])) # Earliest date when mobile time was tracked\n",
    "print()\n",
    "print(np.min(rt_all_clean[['dt']])) # Earliest date whem computer time was tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address both issues at once, we make two modifications to our original process. First, we will only focus on computer time (stored as `rt_computer_clean`) and look at mobile time separately. Next, we will make a modification to our data set after grouping by the DateTime: we *re-index* the grouped data so that every hour from midnight, Feburary 1, 2018 to 11 PM, November 30, 2020 has an entry. (We exclude January since we have incomplete data for that month.) If there is no data recorded for a certain hour, then we fill the `mins` and `hrs` columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hourly_gb_add_missing_hours(rt_data, start='2018-02-01T00:00:00', end='2020-11-30T23:00:00'):\n",
    "    '''\n",
    "    Groups RescueTime data by the `dt` column, then re-indexes it so that every hour from midnight, February 1, 2018 \n",
    "    to 11 PM, November 30, 2020 has a record, setting the `mins` and `hrs` columns to 0 if an hour does not have\n",
    "    any logged time.\n",
    "    '''\n",
    "    all_hours = pd.date_range(start, end, freq='H')\n",
    "    return hourly_gb(rt_data, True).reindex(all_hours, fill_value=0).reset_index().rename(columns={'index' : 'dt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_computer_hourly_missing = hourly_gb_add_missing_hours(rt_computer_clean[['dt', 'mins', 'hrs']])\n",
    "display(rt_computer_hourly_missing) # Notice that DateTimes with no recorded device usage have 0s in the `mins` and `hrs` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed with our original grouping algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add timestamp statistics to our data set\n",
    "rt_computer_hourly_missing_dt = add_dt_stats(rt_computer_hourly_missing)\n",
    "display(rt_computer_hourly_missing_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `date`, `month`, and `year`, and aggregate using the sum function\n",
    "rt_computer_day_month_missing = rt_computer_hourly_missing_dt.groupby(['date', 'month', 'year'], as_index=False).agg(np.sum)[['date', 'month', 'year', 'mins', 'hrs']]\n",
    "display(rt_computer_day_month_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `month` and `year`, and aggregate using the average function\n",
    "rt_computer_month_avg_missing = rt_computer_day_month_missing.groupby(['month', 'year'], as_index=False).agg(np.mean)[['month', 'year', 'hrs']]\n",
    "\n",
    "# Sort by year and month, and add a \"month string\" for plotting purposes\n",
    "rt_computer_month_avg_missing = rt_computer_month_avg_missing.sort_values(['year', 'month'], ignore_index=True)\n",
    "rt_computer_month_avg_missing = rt_computer_month_avg_missing.assign(month_str=pd.to_datetime(rt_computer_month_avg_missing[['month', 'year']].assign(day=np.ones(rt_computer_month_avg_missing.shape[0]))))\n",
    "display(rt_computer_month_avg_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we plot our improved monthly daily average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_month_avg_missing)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the plot of all tracked time, the above plot implies that my computer time usage has not changed much over the years. Generally speaking, I am on my computer for about 4-5 hours per day. \n",
    "\n",
    "Some notes about this plot:\n",
    "- There is a fairly large spike from September 2019 to December 2019 which I will get into if the same pattern appears in the mobile time data set. \n",
    "- Before that, starting October 2018, there is a significant dip in computer usage. I can think of two reasons for this: \n",
    " 1. I may have forgotten to track my time during this period, which is most likely given that one of the months (March 2019) appears as a 0 in the data set.\n",
    " 2. I got an iPad Pro before the start of that semester, so I may have supplanted my computer time with iPad time.\n",
    "- The sharp uptick in the monthly daily average after September 2018 does not appear in this plot. This is evidence that much of that uptick is due to me tracking my mobile time starting that month, and that my actual computer time usage has not changed much since I first started tracking my time.\n",
    "\n",
    "**From now on, we will always add in missing hours and dates to our data set as we did to produce our previous plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create more of these monthly daily average plots, so we'll define functions that will produce the DataFrames required in one procedure. (The `hourly_gb_dt` function will be useful later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_gb_dt(rt_data, start='2018-02-01T00:00:00', end='2020-11-30T23:00:00'):\n",
    "    '''\n",
    "    Returns RescueTime data grouped by hour with timestamp statistics and missing hours.\n",
    "    '''\n",
    "    return add_dt_stats(hourly_gb_add_missing_hours(rt_data, start, end))\n",
    "    \n",
    "def find_monthly_daily_avg(rt_data):\n",
    "    '''\n",
    "    Returns a DataFrame showing the monthly daily average amount of time spent given the data.\n",
    "    '''\n",
    "    df = hourly_gb_dt(rt_data).groupby(['date', 'month', 'year'], as_index=False).agg(np.sum)[['date', 'month', 'year', 'mins', 'hrs']]\n",
    "    df = df.groupby(['month', 'year'], as_index=False).agg(np.mean)[['month', 'year', 'hrs']]\n",
    "    df = df.sort_values(['year', 'month'], ignore_index=True)\n",
    "    return df.assign(month_str=pd.to_datetime(df[['month', 'year']].assign(day=np.ones(df.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to make sure the functions work (compare these to earlier DataFrames)\n",
    "display(hourly_gb_dt(rt_computer_clean), find_monthly_daily_avg(rt_computer_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of the time on my computer was spent doing non-productive activities? In the language of RescueTime, we want to find the daily average amount of time each month I spent on Distracting and Very Distracting activities. In our original data sets, a Distracting activity is indicated by a -1 value in the `Productivity` column, which is the `prod` column in our cleaned sets. A Very Distracting activity is indicated by a -2 in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_all_clean.query('prod == -1 | prod == -2') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activities that are generally regarded as \"distracting\" appear in this subset, like YouTube and news sites. All of the mobile time is also classified as Distracting because more often than not, I'm using my mobile devices for non-productive purposes like playing games (I would say that for every hour of productive work I do on my iPad, I spend four hours doing on my phone playing games or watching videos).\n",
    "\n",
    "Since all mobile time is categorized as Distracting, we don't need to include it in our analysis, so we only need to work with `rt_computer_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show all distracting time on my computer\n",
    "rt_computer_distracting = rt_computer_clean.query('prod == -1 | prod == -2')\n",
    "display(rt_computer_distracting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group this data by hour\n",
    "rt_computer_distracting_hourly = hourly_gb_dt(rt_computer_distracting)\n",
    "display(rt_computer_distracting_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the monthly daily averages spent on distracting activities\n",
    "rt_computer_distracting_monthly_avg = find_monthly_daily_avg(rt_computer_distracting)\n",
    "display(rt_computer_distracting_monthly_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this data along with our overall monthly daily averages\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_month_avg_missing)\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_distracting_monthly_avg)\n",
    "plt.legend(['Total time', 'Distracting and Very Distracting time'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go even more granular. I have recently been worried that I've spent too much time on YouTube. While most of this time was spent on my mobile devices and RescueTime does not track individual websites on them, we can still get a good gauge of the time I spent by looking at the computer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_computer_yt = rt_computer_clean.query(\"act == 'youtube.com'\")\n",
    "rt_computer_yt_monthly_avg = find_monthly_daily_avg(rt_computer_yt)\n",
    "display(rt_computer_yt_monthly_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_month_avg_missing)\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_distracting_monthly_avg)\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_yt_monthly_avg)\n",
    "plt.legend(['Total computer time', 'Distracting and Very Distracting time', 'YouTube'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_all_monthly_avg = find_monthly_daily_avg(rt_all_clean)\n",
    "rt_mobile_monthly_avg = find_monthly_daily_avg(rt_mobile_clean)\n",
    "display(rt_computer_yt_monthly_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot('month_str', 'hrs', data=rt_all_monthly_avg)\n",
    "plt.plot('month_str', 'hrs', data=rt_computer_month_avg_missing)\n",
    "plt.plot('month_str', 'hrs', data=rt_mobile_monthly_avg)\n",
    "plt.legend(['Total time', 'Total computer time', 'Total mobile time'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_all_hourly_dt = hourly_gb_dt(rt_all_clean)\n",
    "rt_mobile_hourly_dt = hourly_gb_dt(rt_mobile_clean)\n",
    "rt_computer_distracting_hourly_dt = hourly_gb_dt(rt_computer_distracting)\n",
    "\n",
    "display(rt_all_distracting_hourly_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rt_all_hourly_time = rt_all_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "rt_mobile_hourly_time = rt_mobile_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "rt_computer_distracting_hourly_time = rt_computer_distracting_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "display(rt_computer_distracting_hourly_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.plot('hour', 'mins', data=rt_all_hourly_time)\n",
    "plt.plot('hour', 'mins', data=rt_mobile_hourly_time)\n",
    "plt.plot('hour', 'mins', data=rt_computer_distracting_hourly_time)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iso = '2020-08-01T00:00:00'\n",
    "end_iso = '2020-11-30T23:00:00'\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_dt = datetime.fromisoformat(start_iso)\n",
    "end_dt = datetime.fromisoformat(end_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_all_distracting_hourly_dt = hourly_gb_dt(rt_all_clean.query('prod == -1 | prod == -2'), start=start_iso, end=end_iso)\n",
    "\n",
    "plt.figure(figsize=(100, 12))\n",
    "plt.suptitle('Average amount of time spent on distracting device activities per hour, by day (' + str(start_dt) + ' to ' + str(end_dt) + ')', fontsize=48, fontweight=1000)\n",
    "\n",
    "for i in np.arange(0, 7):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    df = rt_all_distracting_hourly_dt.query('dow == ' + str(i)).groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "    plt.title(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday' , 'Friday', 'Saturday'][i], fontsize=36)\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylim(0, 60)\n",
    "    plt.ylabel('Amt of time spent (mins)')\n",
    "\n",
    "    plt.plot('hour', 'mins', data=df, linewidth=5)\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "# plt.savefig('rt_distracting_hourly_day_' + start_iso + '_' + end_iso, bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rt_all_distracting_hourly_dt.groupby(['date', 'dow'], as_index=False).agg(np.sum).groupby('dow', as_index=False).agg(np.mean)[['dow', 'hrs']]\n",
    "df2 = rt_all_distracting_hourly_dt.query('hour >= 9 & hour <= 17').groupby(['date', 'dow'], as_index=False).agg(np.sum).groupby('dow', as_index=False).agg(np.mean)[['dow', 'hrs']]\n",
    "df3 = rt_all_distracting_hourly_dt.query('hour >= 22 | hour <= 6').groupby(['date', 'dow'], as_index=False).agg(np.sum).groupby('dow', as_index=False).agg(np.mean)[['dow', 'hrs']]\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Average amount of time spent on distracting device activities per day (' + str(start_dt) + ' to ' + str(end_dt) + ')', fontsize=16, pad=20)\n",
    "plt.xlabel('Day of week', fontsize=14)\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6], labels=['Su', 'M', 'Tu', 'W', 'Th', 'F', 'Sa'])\n",
    "plt.ylabel('Amt of time spent (hrs)', fontsize=14)\n",
    "\n",
    "plt.plot('dow', 'hrs', data=df, label='All time')\n",
    "plt.plot('dow', 'hrs', data=df2, label='Work Hours (9 AM - 5 PM)')\n",
    "plt.plot('dow', 'hrs', data=df3, label='Late Night (10 PM - 6 AM)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iso = '2020-08-01T00:00:00'\n",
    "end_iso = '2020-11-30T23:00:00'\n",
    "\n",
    "start_dt = datetime.fromisoformat(start_iso)\n",
    "end_dt = datetime.fromisoformat(end_iso)\n",
    "\n",
    "rt_all_hourly_dt = hourly_gb_dt(rt_all_clean, start=start_iso, end=end_iso)\n",
    "rt_all_distracting_hourly_dt = hourly_gb_dt(rt_all_clean.query('prod == -1 | prod == -2'), start=start_iso, end=end_iso)\n",
    "rt_all_productive_hourly_dt = hourly_gb_dt(rt_all_clean.query('prod == 1 | prod == 2'), start=start_iso, end=end_iso)\n",
    "\n",
    "df = rt_all_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "df2 = rt_all_distracting_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "df3 = rt_all_productive_hourly_dt.groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Average amount of time spent on activities per hour (' + str(start_dt) + ' to ' + str(end_dt) + ')', fontsize=16, pad=20)\n",
    "plt.xlabel('Hour', fontsize=14)\n",
    "plt.ylabel('Amt of time spent (mins)', fontsize=14)\n",
    "\n",
    "plt.plot('hour', 'mins', data=df, label='All activities')\n",
    "plt.plot('hour', 'mins', data=df2, label='Distracting and Very Distracting activities')\n",
    "plt.plot('hour', 'mins', data=df3, label='Productive and Very Productive activities')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('rt_compare_hourly_' + start_iso + '_' + end_iso, bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_iso = '2020-08-01T00:00:00'\n",
    "end_iso = '2020-11-30T23:00:00'\n",
    "\n",
    "start_dt = datetime.fromisoformat(start_iso)\n",
    "end_dt = datetime.fromisoformat(end_iso)\n",
    "\n",
    "rt_all_hourly_dt = hourly_gb_dt(rt_all_clean, start=start_iso, end=end_iso)\n",
    "rt_all_distracting_hourly_dt = hourly_gb_dt(rt_all_clean.query('prod == -1 | prod == -2'), start=start_iso, end=end_iso)\n",
    "rt_all_productive_hourly_dt = hourly_gb_dt(rt_all_clean.query('prod == 1 | prod == 2'), start=start_iso, end=end_iso)\n",
    "rt_all_yt_hourly_dt = hourly_gb_dt(rt_all_clean.query(\"act == 'youtube.com' | act == 'iOS Device'\"), start=start_iso, end=end_iso)\n",
    "\n",
    "plt.figure(figsize=(100, 12))\n",
    "plt.suptitle('Average amount of time spent on device activities per hour, by day (' + str(start_dt) + ' to ' + str(end_dt) + ')', fontsize=48, fontweight=1000)\n",
    "\n",
    "for i in np.arange(0, 7):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    plt.title(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday' , 'Friday', 'Saturday'][i], fontsize=36)\n",
    "    plt.xlabel('Hour', fontsize=24)\n",
    "    plt.ylim(0, 60)\n",
    "    plt.ylabel('Amt of time spent (mins)', fontsize=24)\n",
    "    \n",
    "    act_labels = ['All activities', 'Distracting and Very Distracting activities', 'YouTube + mobile', 'Productive and Very Productive activities']\n",
    "    j = 0\n",
    "    for df in [rt_all_hourly_dt, rt_all_distracting_hourly_dt, rt_all_yt_hourly_dt, rt_all_productive_hourly_dt]:\n",
    "        df_plot = df.query('dow == ' + str(i)).groupby('hour', as_index=False).agg(np.mean)[['hour', 'mins']]\n",
    "        plt.plot('hour', 'mins', data=df_plot, linewidth=5, label=act_labels[j])\n",
    "        j += 1\n",
    "\n",
    "    if i == 0:\n",
    "        plt.figlegend(fontsize=24, loc='upper left', bbox_to_anchor=(0.72, 1))\n",
    "\n",
    "plt.tight_layout(pad=5.5)\n",
    "plt.savefig('rt_eda_viz/rt_comp_yt_hourly_day_' + start_iso + '_' + end_iso, bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_all_clean[rt_all_clean['act'].str.find('zoom.us') != -1]\n",
    "rt_all_clean.query(\"act == 'youtube.com'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Old Stuff</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add some more useful columns\n",
    "rt_data_actual[\"dt\"] = pd.to_datetime(rt_data_actual[\"Date\"]) # Convert \"Date\" to a DateTime Series\n",
    "rt_data_actual[\"day\"] = rt_data_actual[\"dt\"].dt.date # Extract the day\n",
    "rt_data_actual[\"day_of_week\"] = rt_data_actual[\"dt\"].dt.dayofweek # Extract the day of the week\n",
    "rt_data_actual[\"hour\"] = rt_data_actual[\"dt\"].dt.hour # Extract the hour\n",
    "rt_data_actual[\"mins\"] = rt_data_actual[\"Time Spent (seconds)\"] / 60 # Convert seconds to minutes\n",
    "rt_data_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to query for whatever\n",
    "rt_data_actual.query(\"Date > '2020-01-01'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I want to know is how much time I'm spending on my computer on an hourly basis. My first try is to aggregate the total amount of time I spend per hour by day, then find the mean amount of time per hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, extract just the DateTime, Activity, hour, day of week, and minutes columns from our data set\n",
    "rt_data_time = rt_data_actual[[\"dt\", \"day\", \"hour\", \"day_of_week\", \"Activity\", \"mins\"]]\n",
    "display(rt_data_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do two aggregates: \n",
    "# First, we group by the day and the hour and aggregate using np.sum to get the total amount of time spent per hour.\n",
    "# Then, we group by the hour and aggregate using np.mean to get the average amount of time spent per hour\n",
    "hourly_per_day = rt_data_time.groupby([\"dt\", \"hour\"], as_index=False).agg(np.sum).drop(columns=\"day_of_week\")\n",
    "display(hourly_per_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be some anomalies here: some hours have total device usage *over* 60 minutes! That's physically impossible, so what's going on in these cases? Fortunately, this phenomenon doesn't happen very often, so I can safely ignore this without compromising the rest of my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_per_day.query(\"mins > 60\") # Some anomalies???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_data_actual.query(\"dt == '2019-11-17 12:00:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate hourly totals and averages\n",
    "hourly_gb = hourly_per_day.groupby(\"hour\", as_index=False)\n",
    "hourly_total = hourly_gb.agg(np.sum)\n",
    "hourly_avg = hourly_gb.agg(np.mean)\n",
    "display(hourly_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple line plot of hour vs. average amount of time spent on devices per hour\n",
    "plt.plot(\"hour\", \"mins\", data=hourly_avg)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. According to the above (simplistic) plot, it looks like I'm average more minutes per hour from midnight to five in the morning than *at any time during the actual day.* We can look at specific website time and app usage to see what our culprits may be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at YouTube time specifically\n",
    "yt_hourly_gb = rt_data_time.query(\"Activity == 'youtube.com'\").reset_index(drop=True).groupby([\"Activity\", \"hour\"], as_index=False)\n",
    "yt_hourly_total = yt_hourly_gb.agg(np.sum)\n",
    "yt_hourly_avg = yt_hourly_gb.agg(np.mean)\n",
    "display(yt_hourly_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(\"hour\", \"mins\", data=yt_hourly_avg)\n",
    "plt.plot(hourly_avg[\"mins\"])\n",
    "plt.ylim(0, 60)\n",
    "\n",
    "plt.show();\n",
    "\n",
    "display(yt_hourly_total[\"mins\"].div(hourly_total[\"mins\"])) # Find proportion of each hour's time spent on YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plot and series shows, YouTube accounts for a large portion of my screen time on an hourly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate above plot for any site or app\n",
    "site = \"reddit.com\"\n",
    "site_hourly_gb = rt_data_actual[rt_data_actual[\"Activity\"] == site].reset_index(drop=True).groupby([\"Activity\", \"hour\"], as_index=False)\n",
    "site_hourly_total = site_hourly_gb.agg(np.sum)\n",
    "site_hourly_avg = site_hourly_gb.agg(np.mean)\n",
    "# display(site_hourly)\n",
    "\n",
    "plt.plot(\"hour\", \"mins\", data=site_hourly_avg)\n",
    "plt.plot(hourly_avg[\"mins\"])\n",
    "plt.ylim(0, 60)\n",
    "\n",
    "plt.show();\n",
    "\n",
    "display(site_hourly_total[\"mins\"].div(hourly_total[\"mins\"])) # Find proportion of each hour's time spent on the site/app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this analysis does not account for days where I *don't* have any data logged for a particular hour. We can see this disparity in number of observations per hour if we show how many observations there are for each \"hour\" value in our `hourly_per_day` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_per_day[\"hour\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of this missing data can be attributed to RescueTime not running during certain periods of time (there are a few gaps of several weeks/months in the data set). But generally speaking, RescueTime only records device usage *when the device is actually being used* i.e. if there isn't any activity at all, then RescueTime doesn't record anything. \n",
    "\n",
    "In order to address this issue, we create a pivot table, with the indices being the days (`day`, *not* `dt`) and the columns being the hours. There will be several combinations of day and hour with no data, so we fill those with 0 since most of these combinations represent hours with no computer activity at all. Then, we reset the index and melt on the day, so that we can group the resulting table by the hour and aggregate by mean - this time with the hours with no computer activity properly accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_zero_pivot = rt_data_time.pivot_table(values=\"mins\", index=\"day\", columns=\"hour\", aggfunc=np.sum, fill_value=0).reset_index()\n",
    "display(hourly_zero_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_zero_avg = hourly_zero_pivot.melt(\"day\").groupby(\"hour\", as_index=False).agg(np.mean)\n",
    "display(hourly_zero_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(\"hour\", \"value\", data=hourly_zero_avg)\n",
    "\n",
    "plt.title(\"Amount of time spent on computer, per hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "\n",
    "plt.savefig(\"hourly_avg\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after considering hours with no computer activity, the results still don't make me look any better: although my overall average per hour has dropped significantly, the plot still indicates that I'm using my computer the most around midnight and that my late night usage is pretty much the same as my daytime usage.\n",
    "\n",
    "Now do the same on a per-site basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = \"geometry dash\"\n",
    "site_hourly_zero_pivot = rt_data_time[rt_data_time[\"Activity\"] == site].pivot_table(values=\"mins\", index=\"day\", columns=\"hour\", aggfunc=np.sum, fill_value=0).reset_index()\n",
    "site_hourly_zero_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_hourly_zero_avg = site_hourly_zero_pivot.melt(\"day\").groupby(\"hour\", as_index=False).agg(np.mean)\n",
    "display(site_hourly_zero_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(\"hour\", \"value\", data=hourly_zero_avg)\n",
    "plt.plot(\"hour\", \"value\", data=site_hourly_zero_avg)\n",
    "\n",
    "plt.title(\"Amount of time spent on computer, per hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.legend([\"Total\", site])\n",
    "\n",
    "plt.savefig(\"hourly_avg\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to generate DataFrames for average minutes \n",
    "def hourly_avg(data, mins_column=\"mins\", day_column=\"day\", hour_column=\"hour\"):\n",
    "    pivot = data.pivot_table(values=mins_column, index=day_column, columns=hour_column, aggfunc=np.sum, fill_value=0).reset_index()\n",
    "    df_avg = pivot.melt(day_column).rename(columns={\"value\" : \"mins\"}).groupby(hour_column, as_index=False).agg(np.mean)\n",
    "    return df_avg\n",
    "\n",
    "def daily_avg(data, mins_column=\"mins\", day_column=\"day\", dow_column=\"day_of_week\"):\n",
    "    pivot = data.pivot_table(values=mins_column, index=day_column, columns=dow_column, aggfunc=np.sum, fill_value=0).reset_index()\n",
    "    df_avg = pivot.melt(day_column).rename(columns={\"value\" : \"mins\"}).groupby(dow_column, as_index=False).agg(np.mean)\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up \n",
    "rt_mobile_data_clean = rt_mobile_data_original.drop(columns=[\"Number of People\", \"Activity\", \"Category\", \"Productivity\"])\n",
    "rt_mobile_data_clean[\"dt\"] = pd.to_datetime(rt_mobile_data_clean[\"Date\"])\n",
    "rt_mobile_data_clean[\"day\"] = rt_mobile_data_clean[\"dt\"].dt.date\n",
    "rt_mobile_data_clean[\"day_of_week\"] = rt_mobile_data_clean[\"dt\"].dt.dayofweek\n",
    "rt_mobile_data_clean[\"hour\"] = rt_mobile_data_clean[\"dt\"].dt.hour\n",
    "rt_mobile_data_clean[\"mins\"] = rt_mobile_data_clean[\"Time Spent (seconds)\"] / 60\n",
    "rt_mobile_data_clean = rt_mobile_data_clean.drop(columns=[\"Date\", \"Time Spent (seconds)\"])\n",
    "\n",
    "display(rt_mobile_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily_avg(rt_mobile_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "query_text = \"dt >= '2019-06-01' & dt < '2019-09-01'\"\n",
    "\n",
    "plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_data_time.query(query_text)))\n",
    "plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_mobile_data_clean.query(query_text)))\n",
    "\n",
    "plt.title(\"Amount of time spent on devices, per hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.legend([\"Computer\", \"Mobile\"])\n",
    "\n",
    "# plt.savefig(\"hourly_avg_mobile\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(\"day_of_week\", \"mins\", data=daily_avg(rt_data_time))\n",
    "plt.plot(\"day_of_week\", \"mins\", data=daily_avg(rt_mobile_data_clean))\n",
    "\n",
    "plt.title(\"Amount of time spent on devices, per day\")\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6], labels=['Su', 'M', 'Tu', 'W', 'Th', 'F', 'Sa'])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.legend([\"Computer\", \"Mobile\"])\n",
    "\n",
    "plt.savefig(\"daily_avg_mobile\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_data_time.query(\"day_of_week in [0, 1, 2, 3]\")))\n",
    "plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_data_time.query(\"day_of_week in [4, 5, 6]\")))\n",
    "             \n",
    "plt.title(\"Amount of time spent on computer, per hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.legend([\"First half (Su-W)\", \"Second half (Th-Sa)\"])\n",
    "             \n",
    "plt.savefig(\"hourly_avg_hlfwk\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.ylim(0, 30)\n",
    "plt.title(\"Weekend (Su, Sa)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_mobile_data_clean.query(\"day_of_week in [0, 6]\")))\n",
    "\n",
    "for i in np.arange(1, 6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.ylim(0, 30)\n",
    "    plt.title([\"M\", \"Tu\", \"W\", \"Th\", \"F\"][i - 1])\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Minutes\")\n",
    "    plt.plot(\"hour\", \"mins\", data=hourly_avg(rt_mobile_data_clean.query(\"day_of_week == \" + str(i))))\n",
    "\n",
    "plt.suptitle(\"Amount of time spent on mobile devices, per hour\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
